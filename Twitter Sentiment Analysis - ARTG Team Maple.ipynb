{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARTG Project - By Team Maple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"blue\">Import libraries</font>**  \n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "##To install tweepy uncomment below two statements\n",
    "#import sys\n",
    "#!conda install --yes --prefix {sys.prefix} -c conda-forge tweepy\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"blue\">Step 1: Twitter crawl & downloading data</font>**  \n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'kmflAhDlFLYvVLzUhUTv5Agbw'\n",
    "consumer_secret = 'D8Giq3GMjIMaBQN0Qw63tsoHpajA7hNKy2ouo1XeqQP46SP38C'\n",
    "access_token = '4266439228-U0ySetwuTNEtz3ZGdyPWKVrfIGKY866EMIrzNbN'\n",
    "access_token_secret = 'CVynCeCX22NtK8iygTHGWjota2JhGTfPgk2CO1MUnPfBK'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, timeout=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open/create a file to append data to\n",
    "csvFile = open('testgf.csv', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used csv writer\n",
    "csvWriter = csv.writer(csvFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweepy.Cursor(api.search,\n",
    "                           q=\"#blacklivesmatter\",\n",
    "                           # Data only available for last 7 days. We mined tweets for 4 days - 24,25,26,27 June\n",
    "                           since=\"2020-06-24\",\n",
    "                           # Until 28 means  <28 i.e. <=27\n",
    "                           until=\"2020-06-28\",\n",
    "                           lang=\"en\",\n",
    "                           count=100).items():\n",
    "    # Write a row to the CSV file. We used encode UTF-8\n",
    "    csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])\n",
    "    print(tweet.created_at, tweet.text)\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"blue\">Step 2: Preprocessing the raw dump. Cleaning the tweets</font>**  \n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "##To install preprocessor uncomment the below statement\n",
    "#!conda install tweet-preprocessor\n",
    "import preprocessor as p\n",
    "##To install gensim uncomment the below statement\n",
    "#!conda install gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw tweets mined\n",
    "raw_twts = pd.read_csv('testgf.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the columns\n",
    "raw_twts.columns = ['datetime', 'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a new ID column\n",
    "raw_twts.insert(loc=0, column='id', value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign unique vaues to id column for each row \n",
    "raw_twts[\"id\"] = raw_twts.index + 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates and NaN from the dataframe\n",
    "raw_twts = raw_twts.dropna()\n",
    "raw_twts = raw_twts.drop_duplicates(subset='tweet', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_twts.to_csv ('raw_twts.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          98907\n",
       "datetime    98907\n",
       "tweet       98907\n",
       "dtype: int64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_twts = pd.read_csv('raw_twts.csv')\n",
    "clean_twts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tweet preprocessing first using the tweet-preprocessing library.\n",
    "# Define a preprocess function and use pandas to apply it on each value of 'tweet'\n",
    "def preprocess_tweet(row):\n",
    "    tweet = row['tweet']\n",
    "    tweet = p.clean(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleen tweets.\n",
    "clean_twts['tweet'] = clean_twts.apply(preprocess_tweet, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we apply other manual tweet preprocessing\n",
    "# stop word removal\n",
    "def stopword_removal(row):\n",
    "    tweet = row['tweet']\n",
    "    tweet = remove_stopwords(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_twts['tweet'] = clean_twts.apply(stopword_removal, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove extra white spaces, punctuation and apply lower casing\n",
    "clean_twts['tweet'] = clean_twts['tweet'].str.lower().str.replace('[^\\w\\s]', ' ').str.replace('\\s\\s+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove duplicate tweets again post processing\n",
    "clean_twts = clean_twts.drop_duplicates(subset='tweet', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          93638\n",
       "datetime    93638\n",
       "tweet       93638\n",
       "dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_twts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting\n",
    "clean_twts.to_csv ('clean_twts.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"blue\">Step 3: Uplaoding the clean_twts.csv to a SQLite database</font>**  \n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting csv-to-sqlite\n",
      "  Downloading csv_to_sqlite-2.1.0-py3-none-any.whl (5.6 kB)\n",
      "Requirement already satisfied: click in c:\\users\\prakh\\anaconda3\\lib\\site-packages (from csv-to-sqlite) (7.0)\n",
      "Installing collected packages: csv-to-sqlite\n",
      "Successfully installed csv-to-sqlite-2.1.0\n"
     ]
    }
   ],
   "source": [
    "##To install csv-to-sqlite uncomment the below statement\n",
    "#!pip install csv-to-sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv_to_sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on table clean_twts.csv: \n",
      " table [clean_twts] already exists\n",
      "Written 0 rows into 1 tables in 0.204 seconds\n"
     ]
    }
   ],
   "source": [
    "options = csv_to_sqlite.CsvOptions(typing_style=\"full\", encoding=\"windows-1250\") \n",
    "input_files = [\"clean_twts.csv\"] # pass in the CSV file\n",
    "csv_to_sqlite.write_csv(input_files, \"clean_twts.sqlite\", options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"blue\">Step 4: Using tweets from SQLite database and passing them through IBM Watson Tone Analyzer </font>**  \n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"clean_twts.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('clean_twts',)]\n"
     ]
    }
   ],
   "source": [
    "# find out the table name in the database\n",
    "def sql_fetch(conn):\n",
    "\n",
    "    cursorObj = conn.cursor()\n",
    "\n",
    "    cursorObj.execute('SELECT name from sqlite_master where type= \"table\"')\n",
    "\n",
    "    print(cursorObj.fetchall())\n",
    "\n",
    "sql_fetch(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a tweets dataframe from the database:\n",
    "tweetsdf = pd.read_sql_query(\"SELECT * FROM clean_twts\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "      <td>6/27/2020 23:59</td>\n",
       "      <td>b rt please help cold blooded killer gunned ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000002</td>\n",
       "      <td>6/27/2020 23:59</td>\n",
       "      <td>b rt hi it s over don t let over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000006</td>\n",
       "      <td>6/27/2020 23:59</td>\n",
       "      <td>b rt stop treating communities therapy rage n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000008</td>\n",
       "      <td>6/27/2020 23:59</td>\n",
       "      <td>b rt only protestors nationally actually black...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000010</td>\n",
       "      <td>6/27/2020 23:59</td>\n",
       "      <td>b rt rational people believed all lives matter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id         datetime                                              tweet\n",
       "0  1000000  6/27/2020 23:59  b rt please help cold blooded killer gunned ki...\n",
       "1  1000002  6/27/2020 23:59                  b rt hi it s over don t let over \n",
       "2  1000006  6/27/2020 23:59  b rt stop treating communities therapy rage n ...\n",
       "3  1000008  6/27/2020 23:59  b rt only protestors nationally actually black...\n",
       "4  1000010  6/27/2020 23:59  b rt rational people believed all lives matter..."
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
